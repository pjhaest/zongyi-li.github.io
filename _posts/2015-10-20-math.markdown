---
layout: post
title: Graph Kernel Network for PDEs
date: 2020-03-24 11:59:00-0400
description: Use graph networks to learn the kernel and solve partial differential equations
---
>The blog takes about 10 minutes to read.
It introduces our recent work that uses graph neural networks to learn 
**mappings between function spaces** and solve partial differential equations. 
You can also check out the [paper](https://arxiv.org/abs/2003.03485).
Background of graph neural networks (GNN) and partial differential equations (PDE) are preferred.

### Introduction

A wide range of important engineering and physical problems are governed by 
[PDEs](https://en.wikipedia.org/wiki/Partial_differential_equation). 
Over the past few decades, significant progress has been made on formulating 
and solving  the governing PDEs in many scientific fields 
from micro-scale problems (e.g., quantum and molecular dynamics) to 
macro-scale applications (e.g., civil and marine engineering). 

Despite the success in the application of PDEs to solve real-life problems, 
two significant challenges remain. 
- First, identifying/formulating the underlying PDEs appropriate for 
the modeling of a specific problem usually requires extensive prior knowledge 
in the corresponding field, which is then combined with universal conservation laws 
to design a predictive model; 
for example, modeling the deformation and fracture of solid structures 
requires detailed knowledge on the relationship between stress and strain in the constituent material. 
For complicated systems such as living cells, 
acquiring such knowledge is often elusive 
and formulating the governing PDE for these systems remains prohibitive; 
the possibility of learning such knowledge from data may revolutionize such fields. 
- Second, solving complicated non-linear PDE systems 
(such as those arising in turbulence and plasticity) is computationally demanding; 
again the possibility of using instances of data from such computations 
to design fast approximate solvers holds great potential. 

In both these challenges, if neural networks are to play a role in exploiting 
the increasing volume of available data, 
then there is a need to formulate them so that they are well-adapted 
to mappings from function space to function space.

### Fixed discretization is both good and bad.

PDEs are, unfortunately, hard. 
In general we cannot hope to find analytic solutions to PDEs. 
Hundred years of effort has been made to develop numerical solvers 
such as the finite element method and finite difference method.

<div class="img_row">
    <img  class="col three" src="{{ site.baseurl }}/assets/img/grids.png" alt="" title="Discretizations"/>
</div>
<div class="col three caption">
Three examples of discretization. 
The left one is a regular grid used in the finite difference method;
the middle one is a triangulated grid used in the finite element method;
the right one is a cylinder mesh for real-world airfoil problem.
</div>

Just like how we store images by pixels in *.PNG* and *.JPG* formats, 
we need to discretize the domain of PDEs into some grid and solve the equation on the grid.
It really makes the thing easier.

Nevertheless, there are several drawbacks of discretization:
- The error scales steeply with the resolution. 
We need a high resolution to get good approximations.
- The computation and storage also steeply scale with the resolution (i.e. the size of the grid).
- When the equation is solved on one discretization, 
we cannot change the discretization anymore. 

*.PNG* and *.JPG* formats are good. 
But sometimes maybe we want to save the images as vector images in *.EPS* or *.SVG* formats,
so that it can be used and displayed in any context. 
And for some images, the vector image format is more convenient and efficient.
Similarly, we want to find the continuous version for PDEs, an operator that is invariant of discretization.  

Furthermore, mathematically speaking, such continuous, 
discretization-invariant format is in some sense, closer to the real, analytic solution. 
It has an important mathematical meaning. 
Bear the motivation in mind. Let's develop a rigorous formulation.


### Problem Setting

Consider the standard second order elliptic PDE


$$ - \nabla \cdot (a(x) \nabla u(x))  = f(x), \quad  x \in D $$

$$ u(x) = 0, \quad x \in \partial D $$

for some bounded, open domain $$D \subset \mathbb{R}^d$$ and a fixed source function
$$f$$. This equation is prototypical of PDEs arising in
numerous applications including hydrology  and elasticity. 
For a given function $$a$$, 
the equation has a unique weak solution $$u$$ 
and therefore we can define the solution operator $$ \mathcal{F}_{true}$$
as the map from function to function $$a \mapsto u$$. 

Our goal is to learn a mapping $$\mathcal{F}$$ approximating $$ \mathcal{F}_{true}$$, 
by using a finite collection of observations of input-output pairs 
$$\{a_j, u_j\}_{j=1}^N$$, where each $$a_j$$ and $$u_j$$ are functions on $$D$$.
In practice, the training data is solved numerically or observed in experiments.
In other words, functions $$a_j$$ and $$u_j$$ come with discretization.  
Let $$P_K = \{x_1,\dots,x_K\} \subset D$$ be a $$K$$-point discretization of the domain
$$D$$ and assume we have observations $$a_j|_{P_K}, u_j|_{P_K} $$, for a finite
collection  of input-output pairs indexed by $$j$$. 
We will show how to learn a discretization-invariant mapping based on discretized data.

### Kernel Formulation

For a general PDE of the form

$$ (\mathcal{L}_a u)(x)= f(x), \qquad x \in D\ $$

$$ u(x) = 0, \quad x \in \partial D $$

Under fairly general conditions on $$ \mathcal{L}_a $$, 
we may define the Green's function $$G : D \times D \to \R$$ as the 
unique solution to the problem 

$$ \mathcal{L}_a G(x, \cdot) = \delta_x$$

where $$\delta_x$$ is the delta measure on $$\mathbb{R}^d$$ centered at $$x$$. 
Note that $$G$$ will depend on the coefficient $$a$$ thus we will henceforth denote it as \(G_a\). 
Then operator $$\mathcal{F}_{true}$$ can be written as an integral operator of green function:

$$u(x) = \int_D G_a(x,y)f(y) \: dy$$

Generally the Green's function is continuous at points $$x \neq y$$, 
for example, when $$\mathcal{L}_a$$ is uniformly elliptic. 
Hence it is natural to model the kernel via a neural network $$\kappa$$. 
Just as the Green function, the kernel network $$\kappa$$ takes input $$(x,y)$$.
Since the kernel depends on $$a$$, we let $$\kappa$$ also take input $$(a(x),a(y))$$.

$$u(x) = \int_D \kappa(x,y,a(x),a(y))f(y) \: dy$$

To do the integration, we again need some discretization. The integral is approximated by a sum:

$$u(x) = \frac{1}{K}\sum_y^K \kappa(x,y,a(x),a(y))f(y) $$

### Graph Neural Networks

> Observation: the kernel integral can be viewed as an 

To further take the advantage of neural networks, we will approximate $$u(x)$$
in a high-dimensional representation











