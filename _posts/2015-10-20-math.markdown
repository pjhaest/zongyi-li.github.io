---
layout: post
title: (Under Construction) Graph Kernel Network for PDEs
date: 2020-03-24 11:59:00-0400
description: Use graph networks to learn the kernel and solve partial differential equations
---
>The blog takes about 10 minutes to read.
It introduces our recent work that uses graph neural networks to learn 
**mappings between function spaces** and solve partial differential equations. 
You can also check out the [paper](https://arxiv.org/abs/2003.03485) for more formal derivations.
Background of graph neural networks (GNN) and partial differential equations (PDE) are preferred.

### Introduction

A wide range of important engineering and physical problems are governed by 
[PDEs](https://en.wikipedia.org/wiki/Partial_differential_equation). 
Over the past few decades, significant progress has been made on formulating 
and solving  the governing PDEs in many scientific fields 
from micro-scale problems (e.g., quantum and molecular dynamics) to 
macro-scale applications (e.g., civil and marine engineering). 

Despite the success in the application of PDEs to solve real-life problems, 
two significant challenges remain. 
- First, identifying/formulating the underlying PDEs appropriate for 
the modeling of a specific problem usually requires extensive prior knowledge 
in the corresponding field, which is then combined with universal conservation laws 
to design a predictive model; 
for example, modeling the deformation and fracture of solid structures 
requires detailed knowledge on the relationship between stress and strain in the constituent material. 
For complicated systems such as living cells, 
acquiring such knowledge is often elusive 
and formulating the governing PDE for these systems remains prohibitive; 
the possibility of learning such knowledge from data may revolutionize such fields. 
- Second, solving complicated non-linear PDE systems 
(such as those arising in turbulence and plasticity) is computationally demanding; 
again the possibility of using instances of data from such computations 
to design fast approximate solvers holds great potential. 

In both these challenges, if neural networks are to play a role in exploiting 
the increasing volume of available data, 
then there is a need to formulate them so that they are well-adapted 
to mappings from function space to function space.

### Fixed discretization is both good and bad

PDEs are, unfortunately, hard. 
In general we cannot hope to find analytic solutions to PDEs. 
Hundred years of effort has been made to develop numerical solvers 
such as the finite element method and finite difference method.

<div class="img_row">
    <img  class="col three" src="{{ site.baseurl }}/assets/img/grids.png" alt="" title="Discretizations"/>
</div>
<div class="col three caption">
Three examples of discretization. 
The left one is a regular grid used in the finite difference method;
the middle one is a triangulated grid used in the finite element method;
the right one is a cylinder mesh for real-world airfoil problem.
</div>

Just like how we store images by pixels in *.PNG* and *.JPG* formats, 
we need to discretize the domain of PDEs into some grid and solve the equation on the grid.
It really makes the thing easier.

Nevertheless, there are several drawbacks of discretization:
- The error scales steeply with the resolution. 
We need a high resolution to get good approximations.
- The computation and storage also steeply scale with the resolution (i.e. the size of the grid).
- When the equation is solved on one discretization, 
we cannot change the discretization anymore. 

*.PNG* and *.JPG* formats are good. 
But sometimes maybe we want to save the images as vector images in *.EPS* or *.SVG* formats,
so that it can be used and displayed in any context. 
And for some images, the vector image format is more convenient and efficient.
Similarly, we want to find the continuous version for PDEs, an operator that is invariant of discretization.  

Furthermore, mathematically speaking, such continuous, 
discretization-invariant format is in some sense, closer to the real, analytic solution. 
It has an important mathematical meaning. 
Bear the motivation in mind. Let's develop a rigorous formulation.


### Problem Setting

Consider the standard second order elliptic PDE


$$ - \nabla \cdot (a(x) \nabla u(x))  = f(x), \quad  x \in D $$

$$ u(x) = 0, \quad x \in \partial D $$

for some bounded, open domain $$D \subset \mathbb{R}^d$$ and a fixed source function
$$f$$. This equation is prototypical of PDEs arising in
numerous applications including hydrology  and elasticity. 
For a given function $$a$$, 
the equation has a unique weak solution $$u$$ 
and therefore we can define the solution operator $$ \mathcal{F}_{true}$$
as the map from function to function $$a \mapsto u$$. 

Our goal is to learn a mapping $$\mathcal{F}$$ approximating $$ \mathcal{F}_{true}$$, 
by using a finite collection of observations of input-output pairs 
$$\{a_j, u_j\}_{j=1}^N$$, where each $$a_j$$ and $$u_j$$ are functions on $$D$$.
In practice, the training data is solved numerically or observed in experiments.
In other words, functions $$a_j$$ and $$u_j$$ come with discretization.  
Let $$P_K = \{x_1,\dots,x_K\} \subset D$$ be a $$K$$-point discretization of the domain
$$D$$ and assume we have observations $$a_j|_{P_K}, u_j|_{P_K} $$, for a finite
collection  of input-output pairs indexed by $$j$$. 
We will show how to learn a discretization-invariant mapping based on discretized data.

### Kernel Formulation

For a general PDE of the form:

$$ (\mathcal{L}_a u)(x)= f(x), \quad x \in D $$

$$ u(x) = 0, \quad x \in \partial D $$

Under fairly general conditions on $$ \mathcal{L}_a $$, 
we may define the Green's function $$G : D \times D \to \mathbb{R}$$ as the 
unique solution to the problem 

$$ \mathcal{L}_a G(x, \cdot) = \delta_x$$

where $$\delta_x$$ is the delta measure on $$\mathbb{R}^d$$ centered at $$x$$. 
Note that $$G$$ will depend on the coefficient $$a$$ thus we will henceforth denote it as $$G_a$$. 
Then operator $$\mathcal{F}_{true}$$ can be written as an integral operator of green function:

$$u(x) = \int_D G_a(x,y)f(y) \: dy$$

Generally the Green's function is continuous at points $$x \neq y$$, 
for example, when $$\mathcal{L}_a$$ is uniformly elliptic. 
Hence it is natural to model the kernel via a neural network $$\kappa$$. 
Just as the Green function, the kernel network $$\kappa$$ takes input $$(x,y)$$.
Since the kernel depends on $$a$$, we let $$\kappa$$ also take input $$(a(x),a(y))$$.

$$u(x) = \int_D \kappa(x,y,a(x),a(y))f(y) \: dy$$

### As an Iterative Solver

In our setting, $$f$$ is an unknown but fixed function. 
Instead of doing the kernel convolution with $$f$$, 
we will formulate it as an iterative solver 
that approximated $$u$$ by $$u_t$$,
where $$t = 0,\ldots,T$$ is the time step.

The algorithm starts from an initialization $$u_0$$, 
for which we use $$u_0(x) = (x, a(x))$$.
At each time step $$t$$, it updates $$u_{t+1}$$ by an kernel convolution of $$u_{t}$$.

$$u_{t+1}(x) = \int_D \kappa(x,y,a(x),a(y))u_{t}(x) \: dy$$

It works like an implicit iteration. 
At each iteration the algorithm solves an equation of $$u_{t}(x)$$ and $$u_{t+1}(x)$$
by the kernel integral. $$u_T$$ will be output as the final prediction.

To further take the advantage of neural networks, 
we will lift $$u(x) \in \mathbb{R}^d$$ 
to a high dimensional representation $$v(x) \in \mathbb{R}^n$$,
with $$n$$ the dimension of the hidden representation.

The overall algorithmic framework follow:

$$v_0(x) = NN_1 (x, a(x))$$

$$
v_{t+1}(x) = \sigma\Big( W v_t(x)  
+ \int_{B(x,r)} \!\!\!\kappa_{\phi}\big(x,y,a(x),a(y)\big)
v_t(y)\: \mathrm{d}y \Big)
\quad \text{for } \ t=1,\ldots,T
$$

$$u(x) = NN_2 (v_T (x)) $$

where $$NN_1$$ and $$NN_2$$ are two feed-forward neural networks 
that lifts the initialization to hidden representation $$v$$
and projects the representation back to the solution $$u$$, respective.
$$\sigma$$ is an activation function such as ReLU.
the additional term $$W \in \mathbb{R}^{n \times n}$$ is a linear transformation 
that acts on $v$.
Notice, since the kernel integration happens in the high dimensional representation,
the output of $$\kappa_{\phi}$$ is not a scale, 
but a linear transformation $$W\kappa_{\phi}\big(x,y,a(x),a(y)\big)\in \mathbb{R}^{n \times n}$$.


### Graph Neural Networks

To do the integration, we again need some discretization. 
The integral $$\int_{B(x,r)} \kappa_{\phi}\big(x,y,a(x),a(y)\big)
v_t(y)\: \mathrm{d}y$$ need to be approximated by a sum:

$$ \frac{1}{|N|}\sum_{y \in N(x)} \kappa(x,y,a(x),a(y))v_t(y) $$

> Observation: the kernel integral is equivalent to the message passing on graphs

If you are similar with graph neural network, 
you may have already realized this formulation is the same as 
the aggregation of messages in graph network.
Message passing graph networks comprise a standard architecture employing edge features 
(gilmer et al, 2017). 

If we properly construct graphs on the spatial domain $$D$$ of the PDE, 
the kernel integration can be viewed as an aggregation of messages.
Given node features $$v_t(x) \in \mathbb{R}^{n}$$, 
edge features $$e(x,y) \in \mathbb{R}^{n_e}$$, 
and a graph $$G$$, the message passing neural network with averaging aggregation is

$$ v_{t+1}(x) =  \sigma\Big(W v_t(x) + 
\frac{1}{|N(x)|} \sum_{y \in N(x)} \!\kappa_{\phi}\big(e(x,y)\big) v_t(y)\Big)
$$

where $$W \in \mathbb{R}^{n \times n}$$, 
$$N(x)$$ is the neighborhood of $$x$$ according to the graph, 
$$\kappa_{\phi}\big(e(x,y)\big)$$ is a neural network 
taking as input edge features and as output 
a matrix in $$\mathbb{R}^{n \times n}$$. 
Relating to our kernel formulation, $$e(x,y) = (x,y,a(x),a(y)) $$.


<div class="img_row">
    <img class="col three" src="{{ site.baseurl }}/assets/img/graph.png" title="Graph on the domain"/>
</div>
<div class="col three caption">
</div>


---

<div align="center"> **Under Construction** </div>

---




### Nystrom Approximation

### Example: 1d Poisson
<div class="img_row">
    <img  class="col three" src="{{ site.baseurl }}/assets/img/nik_kernel.png" title="Kernel of 1d Poisson"/>
</div>
<div class="col three caption">
</div>

### Example: 2d Poisson
<div class="img_row">
    <img  class="col three" src="{{ site.baseurl }}/assets/img/GKN_compare.png" title="2d Poisson"/>
</div>
<div class="col three caption">
</div>

### Experiments: generalization of resolution

| Resolutions |      s'=61      |  s'=121 | s'=241 |
|----------|:--------------|:------|:-----|
| s=16 | 0.0717 |  0.0768 | 0.0724 |
| s=31 | 0.0726 |  0.0710 | 0.0722 |
| s=61 | 0.0687 |  0.0728 | 0.0723 |
| s=121 | 0.0687 |  0.0664 | 0.0685 |
| s=241 | 0.0649 |  0.0658 | 0.0649 |

<div class="img_row">
    <img  class="col three" src="{{ site.baseurl }}/assets/img/uai_16to241.png" title="generalization of resolution"/>
</div>
<div class="col three caption">
</div>

### Experiments: comparision of benchmarks

| Networks   |      s=85      |  s'=141 | s'=211 |  s'=421 |
|----------|:--------------|:------|:-----|:-----|
| NN | 0.1716 |  0.1716 | 0.1716 | 0.1716 |
| FCN | 0.0253 |  0.0493 | 0.0727 | 0.1097 |
| PCA+NN | 0.0299 |  0.0298 | 0.0298 | 0.0299 |
| RBM | 0.0244 |  0.0251 | 0.0255 | 0.0259 |
| GKN | 0.0346 |  0.0332 | 0.0342 | 0.0369 |






